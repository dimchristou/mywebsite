---
title: "Improving sparse transformer models for efficient self-attention"
collection: talks
type: "Talk"
permalink: /talks/spacy_irl
venue: "spaCyIRL"
date: 06/07/2019
location: "Berlin, Germany"
---

[![](http://img.youtube.com/vi/KwKr_e7xBQ4/0.jpg)](http://www.youtube.com/watch?v=KwKr_e7xBQ4 "spacy irl")


[Attention layer](https://arxiv.org/abs/1706.03762) is a key component to many state-of-the-art models in Natural Language Processing and Computer Vision. However, it requires quadratic time and memory complexity.
In this talk, factorizations of attention layer in steps are examined. A systematic way for designing and validating such sparsifications is proposed by associating to each factorization a graph and computing the information flow in this graph. Using this method, full information patterns are proposed for an efficient attention layer
